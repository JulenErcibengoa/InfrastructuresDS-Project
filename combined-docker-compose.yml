
networks:
  kafka-net:
    driver: bridge
    name: kafka-net



services:


#######################################################################
#######################################################################
######################## STATIC DATA PIPELINE #########################
#######################################################################
#######################################################################
  hive:
    build: 
      context: ./static-data-pipeline
      dockerfile: Hive/Dockerfile_hive
    container_name: hive
    ports:
      - "10000:10000"  # HiveServer2
      - "10002:10002"  # Hive Metastore
    environment:
      SERVICE_NAME: hiveserver2
    volumes:
      - ./data:/app/data
    networks:
      - kafka-net

  superset:
    build:
      context: ./static-data-pipeline
      dockerfile: Superset/Dockerfile_superset
    container_name: superset
    ports:
      - "8088:8088"
    volumes:
      - ./data:/app/data
      - ./static-data-pipeline/Superset/superset_config.py:/app/pythonpath/superset_config.py
    networks:
      - kafka-net
    depends_on: [hive]
    environment:
      ADMIN_USERNAME: admin
      ADMIN_PASSWORD: admin



#######################################################################
#######################################################################
####################### DYNAMIC DATA PIPELINE #########################
#######################################################################
#######################################################################




  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"

  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.4.0
    container_name: kafka-connect
    depends_on:
      - kafka
      - zookeeper
      - mosquitto
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_GROUP_ID: "mqtt-connect-group"
      CONNECT_CONFIG_STORAGE_TOPIC: "mqtt-connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "mqtt-connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "mqtt-connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.converters.ByteArrayConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_ADVERTISED_PORT: 8083
      CONNECT_REST_PORT: 8083
      CONNECT_PLUGIN_PATH: "/usr/share/confluent-hub-components"
    # Install the MQTT connector via Confluent Hub, then start Kafka Connect:
    command: >
      bash -c 'confluent-hub install --no-prompt confluentinc/kafka-connect-mqtt:latest && /etc/confluent/docker/run'
    ports:
      - "8083:8083"

  mosquitto:
    image: eclipse-mosquitto:2.0.15
    container_name: mosquitto
    ports:
      - "1883:1883"
    volumes:
      - ./stream-data-pipeline/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro

  mqtt_sender:
    build:
      context: ./stream-data-pipeline/mqtt_sender
      dockerfile: Dockerfile
    container_name: mqtt_sender
    depends_on:
      - mosquitto
    volumes:
      - ./data:/app/data



  # druid:
  #   image: apache/druid:33.0.0          # explicit tag â€“ no :latest  :contentReference[oaicite:0]{index=0}
  #   container_name: druid
  #   restart: unless-stopped

  #   # --- ports you may want to reach from the host -------------
  #   ports:
  #     - "8888:8888"   # Router & web console
  #     - "8082:8082"   # Broker (SQL / Superset)
  #     - "8081:8081"   # Coordinator/Overlord API (for ingestion specs)

  #   # --- key env settings --------------------------------------
  #   environment:
  #     # Load Kafka ingestion extension
  #     - druid_extensions_loadList=["druid-kafka-indexing-service"]
  #     # keep everything on local disk + internal Derby metadata (good for dev)
  #     - druid.metadata.storage.type=derby
  #     - druid.storage.type=local
  #     - druid.storage.storageDirectory=/opt/druid/var/segments
  #     # optional: JVM heap for quick-start clusters
  #     - DRUID_JAVA_OPTS=-Xms1g -Xmx1g

  #   depends_on:
  #     - kafka
  #   networks:
  #     - kafka-net